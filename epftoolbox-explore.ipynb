{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electricity price forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import epftoolbox as epf\n",
    "from epftoolbox.data import read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_train, df_test = read_data(\n",
    "    path='data_epf', dataset='FR', begin_test_date='01-01-2016',\n",
    "    end_test_date='01-02-2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, cols_countries, pivot=False):\n",
    "\n",
    "    # get the FR and DE string from cols_FR_DE\n",
    "    countries = [col.split('_')[0] for col in cols_countries]\n",
    "    countries = list(set(countries))\n",
    "    countries.remove('DAH') \n",
    "    # Convert the date to datetime   \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    # Extracting Year, Month, Day, Day of Week, and Hour\n",
    "    df['Year'] = df['date'].dt.year\n",
    "    df['Month'] = df['date'].dt.month\n",
    "    df['Day'] = df['date'].dt.day\n",
    "    df['DayOfWeek'] = df['date'].dt.day_name()\n",
    "    df['Hour'] = df['date'].dt.hour\n",
    "    df['Hour'] = df['Hour'].apply(lambda x: str(x + 1).zfill(2) if x < 23 else '00')\n",
    "    df.insert(0, 'Hour', df.pop('Hour'))\n",
    "    df.insert(0, 'DayOfWeek', df.pop('DayOfWeek'))\n",
    "    df.insert(0, 'Day', df.pop('Day'))\n",
    "    df.insert(0, 'Month', df.pop('Month'))\n",
    "    df.insert(0, 'Year', df.pop('Year'))\n",
    "\n",
    "    # Dataset for FR and DE \n",
    "    \n",
    "    df_FR_DE = df[['Year', 'Month', 'Day', 'DayOfWeek', 'Hour', 'date'] + cols_countries]\n",
    "\n",
    "    # We drop the NaN values \n",
    "    df_FR_DE.dropna(how='all', inplace=True)\n",
    "    \n",
    "    if pivot is True:\n",
    "\n",
    "        # Colonne Y_M_D pour pouvoir grouper les donnÃ©es sur un mÃªme jour\n",
    "        df_FR_DE['Y_M_D'] = df_FR_DE['Year'].astype('str') + \"_\" + df_FR_DE['Month'].astype('str') + \"_\" + df_FR_DE['Day'].astype('str')\n",
    "        df_FR_DE['Y_M_D_H'] = df_FR_DE['Year'].astype('str') + \"_\" + df_FR_DE['Month'].astype('str') + \"_\" + df_FR_DE['Day'].astype('str')+ \"_\" + df_FR_DE['Hour'].astype('str')\n",
    "        # On supprime les duplicates\n",
    "        df_FR_DE.drop_duplicates(subset=[\"Y_M_D_H\"], inplace=True)\n",
    "\n",
    "        # Ajout des colonnes FR_Spot_J1 et DE_Spot_J1\n",
    "        df_FR_DE_J1 = df_FR_DE.copy()[[countries[0]+\"_Spot\", countries[1]+\"_Spot\", \"date\"]]\n",
    "\n",
    "        df_FR_DE_J1[\"date+1\"] = df_FR_DE_J1[\"date\"] + datetime.timedelta(days=1)\n",
    "        df_FR_DE_J1.rename(columns={countries[0]+\"_Spot\":countries[0]+\"_Spot_J1\", countries[1]+\"_Spot\":countries[1]+\"_Spot_J1\"}, inplace=True)\n",
    "        df_FR_DE = pd.merge(df_FR_DE, df_FR_DE_J1.drop(columns=['date']), left_on=\"date\", right_on=\"date+1\")\n",
    "\n",
    "        # Table pivot : 1 ligne = 1 jour avec 24*10 features pour chaque heure de la journÃ©e\n",
    "        df_FR_DE_pivot = pd.pivot_table(df_FR_DE, columns=[\"Hour\"], values=cols_countries+[countries[0]+'_Spot_J1', countries[1]+'_Spot_J1'], index=['Y_M_D'], aggfunc='sum')\n",
    "        df_FR_DE_pivot.columns = df_FR_DE_pivot.columns.to_flat_index()\n",
    "        df_FR_DE_pivot.reset_index(inplace=True)\n",
    "        list_col = df_FR_DE_pivot.columns.values.tolist()\n",
    "        new_cols = []\n",
    "        for col in list_col:\n",
    "            col = str(col)\n",
    "            col = col.replace('(', '').replace(')', '').replace(\",\", \"\").replace(\" \", \"_\").replace(\"'\", \"\")\n",
    "            new_cols.append(col)\n",
    "            \n",
    "        df_FR_DE_pivot.columns = new_cols\n",
    "\n",
    "        df_FR_DE_pivot.dropna(inplace=True)\n",
    "        df_FR_DE = df_FR_DE_pivot\n",
    "        \n",
    "\n",
    "    return df_FR_DE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we load data from Olivier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data\\Data_Europe_DAH_2016_2023.csv', index_col=0, parse_dates=True)\n",
    "# Choose the countries and the columns\n",
    "cols_FR_DE = [\n",
    "    'FR_Spot', 'FR_Load', 'DE_Spot', 'DE_Load',\n",
    "    'FR_DAH_generation', 'DE_DAH_generation',\n",
    "    # 'FR_wind onshore_DAH', 'DE_wind onshore_DAH', \n",
    "    'DAH_ImportCapacity_FR_DE', 'DAH_ImportCapacity_DE_FR'\n",
    "             ]\n",
    "# Pivot = True pour avoir une ligne par jour\n",
    "df_FR_DE = preprocess(df, cols_FR_DE, pivot=False)\n",
    "df_FR_DE.set_index('date', inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = df_FR_DE.index.min()\n",
    "print(min_date)\n",
    "max_date = df_FR_DE.index.max()\n",
    "print(max_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical columns\n",
    "cols = df_FR_DE.columns\n",
    "num_cols = list(df_FR_DE._get_numeric_data().columns)\n",
    "num_cols.remove('Year')\n",
    "num_cols.remove('Month')\n",
    "num_cols.remove('Day')\n",
    "cat_cols = list(set(cols) - set(num_cols))\n",
    "df_FR_DE_num = df_FR_DE[num_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FR_DE_num.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format dataframe with columns (Price, Exogenous 1, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns except DE_Spot\n",
    "cols_FR = [col for col in df_FR_DE_num.columns if col != 'DE_Spot']\n",
    "df_FR = df_FR_DE_num[cols_FR]\n",
    "# list of columns (Price, Exogenous1, Exogenous2, ...)\n",
    "new_cols= ['Price']+[f'Exogenous {i}' for i in range(1, len(cols_FR))]\n",
    "df_FR.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FR.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill missing values with median per month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill na with median of column on 1 month \n",
    "# Resample DataFrame to a monthly frequency\n",
    "df_FR_monthly = df_FR.resample('M').median()\n",
    "# Fill missing values with the median of each column calculated over a rolling window of 1 month\n",
    "df_FR.fillna(df_FR_monthly.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FR.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FR.to_csv('datasets\\FR_readytodnn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test split\n",
    "- 4 years to train\n",
    "- minimum 1 year for test\n",
    "- /!\\ 2021, 2022 prediction, covid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date_test = '2023-09-27'\n",
    "end_date_test = '2023-11-27'\n",
    "df_test = df_FR_DE_num.loc[begin_date_test:end_date_test]\n",
    "df_train = df_FR_DE_num.loc[:begin_date_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data with EPFtoolbox :\n",
    "\n",
    "    'Norm' for normalizing the data to the interval [0, 1].\n",
    "    'Norm1' for normalizing the data to the interval [-1, 1].\n",
    "    'Std' for standarizing the data to follow a normal distribution.\n",
    "    'Median' for normalizing the data based on the median as defined in as defined in here.\n",
    "    'Invariant' for scaling the data based on the asinh transformation (a variance stabilizing transformations) as defined in here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from epftoolbox.data import DataScaler\n",
    "Xtrain = df_train.values\n",
    "Xtest = df_test.values\n",
    "scaler = DataScaler('Norm')\n",
    "Xtrain_scaled = scaler.fit_transform(Xtrain)\n",
    "Xtest_scaled = scaler.transform(Xtest)\n",
    "\n",
    "# Xtrain_inverse = scaler.inverse_transform(Xtrain_scaled)\n",
    "# Xtest_inverse = scaler.inverse_transform(Xtest_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The input features that are considered in Benchmark model\n",
    "\n",
    "#### EPFtoolbox datasets : \n",
    "https://arxiv.org/pdf/2008.08004.pdf\n",
    "\n",
    "Independently of the model, the available input features to forecast the 24 day-ahead prices of day d,\n",
    "i.e. pd = [pd,1, . . . , pd,24], are the same:\n",
    "\n",
    "    - Historical day-ahead prices of the previous three days and one week ago, i.e. pdâˆ’1, pdâˆ’2, pdâˆ’3, pdâˆ’7.\n",
    "\n",
    "    - The day-ahead forecasts of the two variables of interest (see Section 3 for details) for day d available on day d âˆ’ 1, i.e. x1d = [x1d,1, . . . , x1d,24] and x2d = [x2d,1, . . . , x2d,24] ; note that the variables of interest are different for each market.\n",
    "\n",
    "    - Historical day-ahead forecasts of the variables of interest the previous day and one week ago, i.e. x1dâˆ’1,x1dâˆ’7, x2dâˆ’1, x2dâˆ’7.\n",
    "    \n",
    "    - A dummy variable zd that represents the day of the\n",
    "    week. In the case of the linear model, following\n",
    "    the standard practice in the literature [55, 58, 77],\n",
    "\n",
    "Made in function **_build_and_split_XYs**\n",
    "\n",
    "#### SOTA datasets + enriched data with circular encoding information\n",
    "https://hal.science/hal-03621974v1/document\n",
    "\n",
    "Electricity price datasets are a multivariate time series made of daily\n",
    "data. Those datasets can be reconfigured into a (ð‘‹, ð‘Œ ) couple suitable\n",
    "to learn machine learning models. The predictive data is represented\n",
    "by a two dimensional matrix ð‘‹ âˆˆ Rð‘›ð‘‘Ã—ð‘›ð‘ whose rows represent days\n",
    "and columns are ð‘›ð‘ predictive time-dependent values. The values to\n",
    "be predicted correspond to another matrix ð‘Œ âˆˆ Rð‘›ð‘‘Ã—ð‘›ð‘œ , whose rows\n",
    "also stand for the days and columns are the ð‘›ð‘œ day-ahead prices to be\n",
    "predicted: ð‘Œð‘‘ =(ð‘Œ1ð‘‘+1, â€¦, ð‘Œ ð‘›0ð‘‘+1).\n",
    "\n",
    "To model the time series aspect of the features, ð‘‹ includes the prices of the current day, those of the day\n",
    "before, two days before and the previous week (1, 2, 3 and 7 days lag).\n",
    "Exogenous features are included for the day, the day before and the\n",
    "previous week. In addition to these 240 characteristics, the day of the\n",
    "week is also encoded as an integer and added to the matrix ð‘‹. Indeed,\n",
    "electricity prices are non-stationary time series and exhibit seasonal\n",
    "trends captured by this additional feature. All features (prices and\n",
    "exogenous) are provided with hourly granularity. Thus, the predictive\n",
    "matrix ð‘‹ is as follows:\n",
    "ð‘‹ð‘‘ =(ð‘Œð‘‘âˆ’1, ð‘Œð‘‘âˆ’2, ð‘Œð‘‘âˆ’3, ð‘Œð‘‘âˆ’7, ð¸1ð‘‘, ð¸1ð‘‘âˆ’1, ð¸1ð‘‘âˆ’7,ð¸2ð‘‘, ð¸2ð‘‘âˆ’1, ð¸2ð‘‘âˆ’7,DayOfWeek) with ð‘›ð‘=241.\n",
    "\n",
    "In order to forecast 24-hour prices for the next day, the datasets are\n",
    "reshaped so that for one day ð‘‘, ð‘Œð‘‘\n",
    "contains all 24 prices for the next\n",
    "day: ð‘Œð‘‘ =(ð‘Œ1ð‘‘+1, â€¦, ð‘Œ 24ð‘‘+1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization\n",
    "For the DNN, as in the original study [59], the input\n",
    "features are optimized together with the hyperparameters using the tree Parzen estimator [108] (see Section\n",
    "4.3 for details).\n",
    "\n",
    "As in the original DNN paper [59], the hyperparameters\n",
    "and input features are optimized together using the treestructured Parzen estimator [108], a Bayesian optimization\n",
    "algorithm based on sequential model-based optimization.\n",
    "To do so, the features are modeled as hyperparameters,\n",
    "with each hyperparameter representing a binary variable\n",
    "that selects whether or not a specific feature is included in\n",
    "the model (as explained in [43]). In more detail, to select\n",
    "which of the 241 available input features are relevant, the\n",
    "method employs 11 decision variables, i.e. 11 hyperparameters:\n",
    "\n",
    "    - Four binary hyperparameters (1-4) that indicate\n",
    "    whether or not to include the historical day ahead\n",
    "    prices pdâˆ’1, pdâˆ’2, pdâˆ’3, pdâˆ’7. The selection is done\n",
    "    per day12, e.g. the algorithm either selects all the\n",
    "    prices pdâˆ’j of j days ago or it cannot select any price\n",
    "    from day d âˆ’ j, hence the four hyperparameters.\n",
    "\n",
    "    - Two binary hyperparameters (5-6) that indicate\n",
    "    whether or not to include each of the day-ahead forecasts x1d and x2d. As with the past prices, this is done for the whole day, i.e. a hyperparameter either selects all the elements in xjd or none.\n",
    "\n",
    "    - Four binary hyperparameters (7-10) that indicate\n",
    "    whether or not to include the historical day-ahead\n",
    "    forecasts x1dâˆ’1, x2dâˆ’1, x1dâˆ’7, and x2dâˆ’7. This selection is also done per day.\n",
    "\n",
    "    - One binary hyperparameter (11) that indicates\n",
    "    whether or not to include the variable zd representing\n",
    "    the day of the week.\n",
    "In short, 10 binary hyperparameters indicating whether or\n",
    "not to include 24 inputs each and another binary hyperparameter indicating whether or not to include a dummy\n",
    "variable.\n",
    "\n",
    "Besides selecting the features, the algorithm also optimizes eight additional hyperparameters:\n",
    "1) the number of neurons per layer\n",
    "2) the activation function\n",
    "3) the dropout rate\n",
    "4) the learning rate\n",
    "5) whether or not to\n",
    "use batch normalization\n",
    "6) the type of data preprocessing technique\n",
    "7) the initialization of the DNN weights\n",
    "8) the coefficient for L1 regularization that is applied to each\n",
    "layerâ€™s kernel.\n",
    "\n",
    "Unlike the weights of the DNN that are recalibrated on a\n",
    "daily basis, the hyperparameter and features are optimized\n",
    "only once using the four years of data prior to the testing\n",
    "period. \n",
    "\n",
    "It is important to note that the algorithm runs\n",
    "for a number T of iterations, where at every iteration the\n",
    "algorithm infers a potential optimal subset of hyperparameters/features and evaluates this subset in the validation dataset.\n",
    "\n",
    "For the proposed open-access benchmark models, T is selected as 1500 iterations to obtain a trade-off\n",
    "between accuracy and computational requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"datasets/FR_readytodnn.csv\", index_col=0, parse_dates=True)\n",
    "min_date = X.index.min()\n",
    "print(min_date)\n",
    "max_date = X.index.max()\n",
    "print(max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from epftoolbox.models import hyperparameter_optimizer\n",
    "\n",
    "# Number of layers in DNN\n",
    "nlayers = 2\n",
    "\n",
    "# Market under study. If it not one of the standard ones, the file name\n",
    "# has to be provided, where the file has to be a csv file\n",
    "dataset = 'FR_readytodnn'\n",
    "\n",
    "# Number of years (a year is 364 days) in the test dataset.\n",
    "years_test = 1\n",
    "\n",
    "# Optional parameters for selecting the test dataset, if either of them is not provided, \n",
    "# the test dataset is built using the years_test parameter. They should either be one of\n",
    "# the date formats existing in python or a string with the following format\n",
    "# \"%d/%m/%Y %H:%M\"\n",
    "begin_test_date = \"2022-12-31\"\n",
    "end_test_date = \"2023-12-31\"\n",
    "\n",
    "# Boolean that selects whether the validation and training datasets are shuffled\n",
    "shuffle_train = 1\n",
    "\n",
    "# Boolean that selects whether a data augmentation technique for DNNs is used\n",
    "data_augmentation = 0\n",
    "\n",
    "# Boolean that selects whether we start a new hyperparameter optimization or we restart an existing one\n",
    "new_hyperopt = 1\n",
    "\n",
    "# Number of years used in the training dataset for recalibration\n",
    "calibration_window = 4\n",
    "\n",
    "# Unique identifier to read the trials file of hyperparameter optimization\n",
    "experiment_id = 1\n",
    "\n",
    "# Number of iterations for hyperparameter optimization\n",
    "# It can be empirically observed that the performance of the models barely improves after 1000 iterations. \n",
    "# Moreover, performing 1500 iteration takes approximately just one day on a regular quadcore\n",
    "# laptop like the i7-6920HQ, a computation cost very acceptable when\n",
    "# the algorithm has to run only once\n",
    "max_evals = 1500\n",
    "\n",
    "path_datasets_folder = \"./datasets/\"\n",
    "path_hyperparameters_folder = \"./experimental_files/\"\n",
    "\n",
    "\n",
    "# Check documentation of the hyperparameter_optimizer for each of the function parameters\n",
    "# In this example, we optimize a model for the PJM market.\n",
    "# We consider two directories, one for storing the datasets and the other one for the experimental files.\n",
    "# We start a hyperparameter optimization from scratch. We employ 1500 iterations in hyperopt,\n",
    "# 1 year of test data, a DNN with 2 hidden layers, a calibration window of 4 years,\n",
    "# we avoid data augmentation,  and we provide an experiment_id equal to 1\n",
    "hyperparameter_optimizer(path_datasets_folder=path_datasets_folder, \n",
    "                         path_hyperparameters_folder=path_hyperparameters_folder, \n",
    "                         new_hyperopt=new_hyperopt, max_evals=max_evals, nlayers=nlayers, dataset=dataset, \n",
    "                         years_test=years_test, calibration_window=calibration_window, \n",
    "                         shuffle_train=shuffle_train, data_augmentation=0, experiment_id=experiment_id,\n",
    "                         begin_test_date=begin_test_date, end_test_date=end_test_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checklist to ensure adequate EPF research :\n",
    "\n",
    "As a final contribution, and with the goal of facilitating\n",
    "the work of reviewers of future EPF publications, we provide a short checklist to evaluate whether any new research in EPF satisfies the requirements to be reproducible and to lead to meaningful conclusions:\n",
    "\n",
    "    â€¢ The test dataset comprises at least a year of data.\n",
    "    â€¢ Any new model is tested against state-of-the-art open-access models, e.g. the ones provided here.\n",
    "    â€¢ The computational cost of new methods is evaluated\n",
    "    and compared against the computational cost of existing methods.\n",
    "    â€¢ The employed datasets are open-access.\n",
    "    â€¢ The study is based on multiple markets.\n",
    "    â€¢ rMAE is employed as one of the accuracy metrics to\n",
    "    evaluate forecasting accuracy.\n",
    "    â€¢ Statistical testing is used to assess whether differences\n",
    "    in performance are significant.\n",
    "    â€¢ Forecasting models are recalibrated on a daily basis\n",
    "    and not simply estimated once and evaluated in the\n",
    "    full out-of-sample dataset.\n",
    "    â€¢ Hyperparameters are estimated using a validation\n",
    "    dataset that is different from the test dataset.\n",
    "    â€¢ The split and dates of the dataset are explicitly stated.\n",
    "    â€¢ All the inputs of the model are explicitly defined\n",
    "    - The test dataset is selected as the last section of the\n",
    "    full dataset and does not contain any overlapping data\n",
    "    with the training or validation datasets.\n",
    "    â€¢ State-of-the-art and free toolboxes are used for modeling the benchmark models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
